# âš¡ End-to-End RAG Application

Eine minimalistische, lokale **RAG-Applikation** (Retrieval-Augmented Generation) zur Analyse von PDF-Dokumenten.

Das System ermÃ¶glicht es, PDFs hochzuladen und Fragen zum Inhalt zu stellen. Die Antworten werden durch ein lokales LLM (Mistral) generiert, basierend auf den tatsÃ¤chlich gefundenen Textstellen im Dokument â€“ **ohne Halluzinationen** und **ohne Daten-Upload in die Cloud**.

![App Screenshot](https://i.imgur.com/ougv3h1.png)


---

## ğŸš€ Tech Stack

* **Backend:** FastAPI (Python)
* **Frontend:** Streamlit
* **AI/ML:** LangChain, Ollama (Mistral), HuggingFace Embeddings
* **Datenbank:** ChromaDB (Vektordatenbank)
* **Container:** Docker & Docker Compose

---

## ğŸ› ï¸ Voraussetzungen

Bevor du startest, stelle sicher, dass folgende Tools installiert sind:

1.  **Docker & Docker Compose** (fÃ¼r die Container-Umgebung)
2.  **Ollama** (muss lokal auf deinem Rechner laufen)

### Ollama Setup (Wichtig!)
Da das LLM lokal lÃ¤uft, muss Ollama auf deinem Host-System gestartet sein:

1.  Installiere [Ollama](https://ollama.com/).
2.  Lade das Modell herunter:
    ```bash
    ollama pull mistral
    ```
3.  Starte den Server (falls er nicht lÃ¤uft):
    ```bash
    ollama serve
    ```

---

## ğŸ Starten der Anwendung

Das gesamte System (Backend + Frontend) lÃ¤sst sich mit einem einzigen Befehl starten:

```bash
docker compose up --build
